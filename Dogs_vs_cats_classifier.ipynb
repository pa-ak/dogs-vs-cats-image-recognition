{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image resizing and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The dataset of dogs & cats images was downloaded from Kaggle https://www.kaggle.com/datasets/chetankv/dogs-cats-images\n",
    "\n",
    "## Using OpenCV we: \n",
    "## a) Resize all the images and \n",
    "## b) Normalize the pixel values of the images. Normalization is a process that scales the pixel values \n",
    "## of an image to a specific range. It helps to improve the performance of the ML model \n",
    "## by reducing the scale of the input data.\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the new size\n",
    "new_size = (256, 256)\n",
    "\n",
    "# Define the path to the dataset folder\n",
    "dataset_path = \"dataset/\"\n",
    "\n",
    "# Loop through the training and test sets\n",
    "for set_name in [\"training_set\", \"test_set\"]:\n",
    "    for class_name in [\"cats\", \"dogs\"]:\n",
    "        # Define the path to the class folder\n",
    "        path = dataset_path + set_name + \"/\" + class_name + \"/\"\n",
    "        # Loop through all the images in the class folder\n",
    "        for filename in os.listdir(path):\n",
    "            # Load the image\n",
    "            img = cv2.imread(path + filename)\n",
    "            # Resize the image\n",
    "            img = cv2.resize(img, new_size)\n",
    "            # Normalize the pixel values\n",
    "            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            # Save the resized image\n",
    "            cv2.imwrite(path + filename, img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data augmentation\n",
    "\n",
    "Data augmentation is a technique that is used to artificially increase the size of the training dataset by creating new, modified versions of the existing images. This helps to improve the performance of the machine learning model by increasing the diversity of the training data and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## This code uses the Keras ImageDataGenerator class to perform data augmentation on the images, \n",
    " ## and for each existing image it generates 5 new images using different combinations of data augmentation \n",
    " ## options such as rotation, shift, zoom, and flipping.\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import scipy\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Define the path to the dataset folder\n",
    "dataset_path = \"dataset/\"\n",
    "\n",
    "# Define the data generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Loop through the training and test sets\n",
    "for set_name in [\"training_set\", \"test_set\"]:\n",
    "    for class_name in [\"cats\", \"dogs\"]:\n",
    "        # Define the path to the class folder\n",
    "        path = dataset_path + set_name + \"/\" + class_name + \"/\"\n",
    "        # Loop through all the images in the class folder\n",
    "        for filename in os.listdir(path):\n",
    "            # Load the image\n",
    "            img = cv2.imread(path + filename)\n",
    "            # Reshape the image\n",
    "            img = img.reshape((1,) + img.shape)\n",
    "            # Generate new images\n",
    "            i = 0\n",
    "            for batch in datagen.flow(img, batch_size=1, save_prefix=class_name + '_' + filename, save_format='jpg', save_to_dir=path):\n",
    "                i += 1\n",
    "                if i > 5:\n",
    "                    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building, Training and Testing a Neural Network for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We build a simple convolutional neural network (CNN) model for classifying images of cats and dogs\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add convolutional layers\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten the output data for the dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a dense layer for classification\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# Add a final sigmoid layer for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 55989 images belonging to 2 classes.\n",
      "Found 13995 images belonging to 2 classes.\n",
      "Epoch 1/5\n",
      "1750/1750 [==============================] - 3534s 2s/step - loss: 0.6041 - accuracy: 0.6613\n",
      "Epoch 2/5\n",
      "1750/1750 [==============================] - 3944s 2s/step - loss: 0.4865 - accuracy: 0.7636\n",
      "Epoch 3/5\n",
      "1750/1750 [==============================] - 4097s 2s/step - loss: 0.3916 - accuracy: 0.8196\n",
      "Epoch 4/5\n",
      "1750/1750 [==============================] - 3870s 2s/step - loss: 0.2595 - accuracy: 0.8906\n",
      "Epoch 5/5\n",
      "1750/1750 [==============================] - 3869s 2s/step - loss: 0.1296 - accuracy: 0.9503\n",
      "438/438 [==============================] - 317s 723ms/step - loss: 0.7955 - accuracy: 0.7655\n",
      "Test accuracy: 0.7654876708984375\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "#tf.function\n",
    "def train_and_evaluate(model, datagen):\n",
    "    # Load the preprocessed data\n",
    "    train_data = datagen.flow_from_directory(directory='dataset/training_set', target_size=(256, 256), batch_size=32, class_mode='binary')\n",
    "    test_data = datagen.flow_from_directory(directory='dataset/test_set', target_size=(256, 256), batch_size=32, class_mode='binary')\n",
    "\n",
    "    # Train the model on the new data for a few epochs\n",
    "    model.fit(train_data, epochs=5, batch_size=64)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_acc = model.evaluate(test_data)\n",
    "    print(\"Test accuracy:\", test_acc)\n",
    "\n",
    "# call the function with the model and data generators\n",
    "train_and_evaluate(model, datagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b7e9cb8e453d6cda0fe8c8dd13f891a1f09162f0e7c66ffeae7751a7aecf00d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
